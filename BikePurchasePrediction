library(ggplot2)
library(dplyr)
library(repr)
library(caret)
library(eeptools)    # for DOB to age conversion
library(ROCR)
library(pROC)        # for AUC and ROC 

#1. Extraction of Data
test_f <- read.csv("AW_test.csv", header = TRUE, stringsAsFactor= TRUE)
adventure= read.csv('AdvWorksCusts.csv', header= TRUE, stringsAsFactors= TRUE)
avgMonthSpend = read.csv('AW_AveMonthSpend.csv', header= TRUE, stringsAsFactors = FALSE)
bikebuyer= read.csv('AW_BikeBuyer.csv', header= TRUE, stringsAsFactors= FALSE)

str(test)

#removing unuseful columns like Names,and 'CustomerID'
test[,'CustomerID'] <- NULL
test[,1:5] <- NULL
test[,'PhoneNumber'] <- NULL

str(test)
#checking for duplicates with a boolean argument.
dim(test)==dim(distinct(test))

lapply(test, function(x) {any(x=='?')})  # Checking any missing values.


#NOTE:  the three data set in previous set have different duplicates, so, 
# I am not removing duplicates for merging
table(adventure[,'CustomerID']==bikebuyer[,'CustomerID'])

#Merging columns
adventure$BikeBuyer <- bikebuyer$BikeBuyer
#adventure$AveMonthSpend <- avgMonthSpend$AveMonthSpend

#DOB to age conversion
adventure$BirthDate <- as.Date(adventure$BirthDate)
adventure$BirthDate <- round(age_calc(adventure$BirthDate, unit='years'))

adventure$PostalCode <- as.numeric(adventure$PostalCode) 
#I found out that there are over 3100 NA , so I removed the column
adventure$PostalCode <- NULL

dim(distinct(adventure))
adventure <- distinct(adventure)     # Removing the duplicates

adventure[, 1:8] <- NULL 		#removing unuseful columns like name and ID Number
adventure$PhoneNumber <- NULL
# Since City has 270 levels which is a lot compared to the data size, so I 
#deleted 'City' column as well.
adventure[,'City'] <- NULL

##### extracting numerical columns
A <- length(names(adventure)) 
feature <- numeric(A)          #creating zero vector of length equal to the column in 'Credit'
for(i in 1:A){
   feature[i]= ifelse(is.numeric(adventure[,i])==TRUE, 1,NA)
   names(feature)[i]=names(adventure[i])
}
feature <- feature[complete.cases(feature)]
num_cols <- names(feature)
num_cols <- num_cols[1:6]   # add BirthDate to this as well.
num_cols[6] <- c('BirthDate')

adventure[,'BikeBuyer']<- ifelse(adventure[,'BikeBuyer']== 1,'sale','nosale')
adventure[,'BikeBuyer']<- factor(adventure[,'BikeBuyer'], level=c('sale','nosale'))
head(adventure[,'BikeBuyer'])

#Saving the formatted data so far.
write.csv(adventure, "AW_bike_Preped.csv", row.names= FALSE)


# Partition and Scaling
partition <- createDataPartition(adventure[,'BikeBuyer'], times=1, p=0.5, list= FALSE)
training <- adventure[partition,]
test 	<- adventure[-partition,]


preProcValues <- preProcess(adventure[, num_cols], method=c('center', 'scale'))
adventure[,num_cols] <- predict(preProcValues, adventure[,num_cols])
training[,num_cols] <- predict(preProcValues, training[,num_cols])
test[,num_cols] <- predict(preProcValues, test[,num_cols])


#Setting up linear reg. model

set.seed(1234)
logistic_mod= glm(BikeBuyer ~ BirthDate + HomeOwnerFlag + NumberCarsOwned + NumberChildrenAtHome+
					TotalChildren + YearlyIncome + BirthDate +Gender + MaritalStatus +
					Education+ CountryRegionName,
			family = binomial,
			data = training)

logistic_mod$coefficients

test$probs = predict(logistic_mod, newdata=test, type='response')

threshold = 0.4
test$score= ifelse(test$probs > threshold, 'nosale','sale')

#ALTERNATIVELY

score_model = function(df,threshold){
	df$score = ifelse(df$probs < threshold, 'sale','nosale')
	df
}

test = score_model(test,0.5)
table(test$BikeBuyer==test$score)

# The regression model is not very effective with many wrong prediction/score.

#COMPUTING  a weighted model
weights = ifelse(training$BikeBuyer == 'sale', 0.66,0.34)

#SECOND  regression model
logistic_mod_w = glm(BikeBuyer ~ HomeOwnerFlag + NumberCarsOwned + NumberChildrenAtHome+
					TotalChildren + YearlyIncome + BirthDate +Gender + MaritalStatus +
					Education + CountryRegionName,
			family = quasibinomial,
			weights= weights,
			data = training)


test$probs = predict(logistic_mod_w, newdata=test,type= 'response'  )

test = score_model(test,0.5)
test[1:10, c('BikeBuyer','probs','score')]

table(test$BikeBuyer==test$score)

# Evaluating the logistics:

logistic.eval <- function(df){
 #1. Creating  a confusion matrix
 df$conf = ifelse(df$BikeBuyer =='sale' & df$score == 'sale', 'TP',
		ifelse(df$BikeBuyer == 'sale' & df$score == 'nosale', 'FN',
			ifelse(df$BikeBuyer == 'nosale' & df$score == 'nosale', 'TN','FP')))

 #2. Elements of confusion matrix 
 TP= length(df[df$conf == 'TP', 'conf'])
 FP= length(df[df$conf == 'FP', 'conf'])
 TN= length(df[df$conf == 'TN', 'conf'])
 FN= length(df[df$conf == 'FN', 'conf'])

 #3. Confusion matrix as a data frame
 out= data.frame(Negative= c(TN,FN), Positive=c(FP,TP))
 row.names(out)= c('Actual Negative','Actual Positive')
 print(out)

 #4. Compute and print metrics
 P = TP/(TP +FP)
 A = (TP+TN)/(TP+TN+FP+FN)
 R = TP/(TP +FN)
 F1= 2*(P*R)/(P+R)
 S = TN/(TN+FP)
 cat('\n')
 cat(paste('Accuracy	=',as.character(round(A, 3)),'\n'))
 cat(paste('Precision	=',as.character(round(P, 3)),'\n'))
 cat(paste('Recall	=',as.character(round(R, 3)),'\n'))
 cat(paste('F1		=',as.character(round(F1,3)),'\n'))
 cat(paste('Specificity	=',as.character(round(S, 3)),'\n'))

 roc_obj <- roc(df$BikeBuyer, df$probs)
 cat(paste('AUC		=',as.character(round(auc(roc_obj),3)),'\n'))
}

logistic.eval(test)

#Though Precision is fairly good, Recall is low. 


## 
#A_City <- adventure[,1:2]
#adventure[,'City'] <-numeric(nrow(adventure))

#############################################################
############	DIMENSIONALITY REDUCTION         ##########


dummies <- dummyVars(BikeBuyer ~ ., data = adventure)

training_label = adventure[partition, 'BikeBuyer'] # Subset training label
training = predict(dummies, newdata= training)    # transform categorical to dummy vars

test_label = adventure[-partition, 'BikeBuyer']
test= predict(dummies, newdata= test)
dim(training); dim(test)
head(test, n=4)

#### Computing the Principal Components
pca_adventure = prcomp(training)

# Variance for each component
var_exp = pca_adventure$sdev**2/sum(pca_adventure$sdev**2)
var_exp                 # Notice the variance is in decreasing order
sum(var_exp)

#Creating a plot
plot_scree = function(pca_mod){
 ##Plot as variance explained
 df= data.frame(x= 1:length(var_exp), y=var_exp)
 ggplot(df, aes(x,y)) + geom_line(size=1, color='blue') +
 xlab('Component number') + ylab('Variance explained') +
 ggtitle('Scree Plot of variance explained vs. Prin. Components')
# head(df)
}

plot_scree(pca_adventure)
#The first few components explain a large fraction of the variance and thus 
#contain much of the explanatory  information in the data. 

## Creating a PCA model with reduced number of components. 

#Computing first 10 components.
pca_adventure_10 = prcomp(training, rank =10)

#Scale the eigen values
var_exp_10= pca_adventure_10$sdev**2 /sum(pca_adventure_10$sdev**2)
Nrow= nrow(pca_adventure_10$rotation)
Ncol= ncol(pca_adventure_10$rotation)       # number of components selected
scaled_pca_10 = data.frame(matrix(rep(0, Nrow*Ncol), nrow = Nrow, ncol=Ncol))
# creating an empty zero matrix with number of columns equal to components selected


##Scaling the rotations
for(i in 1:Nrow){
 scaled_pca_10[i,]= pca_adventure_10$rotation[i,]*var_exp_10[1:Ncol]
}
dim(scaled_pca_10)
pca_adventure_10$rotation[1:10,]


#Computing and evaluating logistic regression model using features 
#transformed by first 10 Principal Components

training_10 = training %*% as.matrix(scaled_pca_10)
dim(training_10)

training_10 = data.frame(training_10)
training_10[,'BikeBuyer'] = training_label

#Creating a weight vector for training cases
weights = ifelse(training_10$BikeBuyer == 'sale', 0.66,0.34)

##Define and fit logistic reg. model
set.seed(1234)
logistic_mod_10= glm(BikeBuyer ~ ., data = training_10,
			weights= weights,
			family = quasibinomial)
logistic_mod_10$coefficients

test_10 = test %*% as.matrix(scaled_pca_10)
test_10 = data.frame(test_10)
test_10[, 'BikeBuyer']= test_label

#Scoring the model
test_10$probs = predict(logistic_mod_10, newdata= test_10, type='response')
test_10 = score_model(test_10, 0.4)
test_10[1:10,c('BikeBuyer','probs','score)]


###Adding ROC_AUC to logistic.eval()

ROC_AUC= function(df){
 options(repr.plot.width=5, repr.plot.height=5)
 pred_obj = prediction(df$probs, df$BikeBuyer)
 perf_obj = performance(pred_obj, measure = 'tpr', x.measure= 'fpr')
 AUC = performance(pred_obj, 'auc')@y.values[[1]] 
  		#accessing the AUC from slot of the S4 object
 plot(perf_obj)
 abline(a=0, b=1, col='red')
 text(0.8, 0.2, paste('AUC =', as.character(round(AUC,3)))) 
}

ROC_AUC(test_10)

# RECALL, Precision and F1 have reasonably well values.

##Computing for first 20 Principal Components.

pca_adventure_20 = prcomp(training, rank=20)

var_exp_20 = pca_adventure_20$sdev**2/sum(pca_adventure_20$sdev**2)
Nrow= nrow(pca_adventure_20$rotation)
Ncol= ncol(pca_adventure_20$rotation)
scaled_pca_20 = data.frame(matrix(rep(0, Nrow*Ncol), nrow= Nrow, ncol= Ncol))

##Scaling the notations
for(i in 1:Nrow){
 scaled_pca_20[i,]= pca_adventure_20$rotation[i,]*var_exp_20[1:Ncol]
}
dim(scaled_pca_20)

training_20 = training%*% as.matrix(scaled_pca_20)
training_20 = data.frame(training_20)
training_20[,'BikeBuyer']= training_label

logistic_mod_20 = glm(BikeBuyer ~ ., data= training_20,
				weights= weights,
				family= quasibinomial)

logistic_mod_20$coefficients

test_20 = test %*%as.matrix(scaled_pca_20)
test_20 = data.frame(test_20)
test_20[, 'BikeBuyer'] = test_label

#Scoring the model
test_20$probs = predict(logistic_mod_20 , newdata= test_20, type='response')
test_20 = score_model(test_20, 0.4)

#Evaluate the model
logistic.eval(test_20)
ROC_AUC(test_20)


#Shows a little improvement in performance of Recall.  

### Applying 20 component analysis on the whole data set.




#Getting test data ready "test_f"
test_f[1:9]= NULL
test_f[,'PhoneNumber']= NULL
test_f[,'StateProvinceName']= numeric(nrow(test_f))
test_f[,'PostalCode']= NULL

#DOB to age conversion
library(lubridate)

BirthDate <- test_f$BirthDate
test_f$BirthDate <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-as.numeric(format(as.Date(test_f$BirthDate,format="%m/%d/%Y"),format="%Y"))
# since the DOB is writted in mm/dd/yy format , the previous formula for 
# DOB in the form yy-dd-mm didn't work here. 

## Alternativey,  we can use
#  test$BirthDate<- as.Date(test$BirthDate, format="%m/%d/%Y")   # first converts date into "Y-m-d" format
# test$BirthDate <- round(age_calc(test$BirthDate, unit="years"))

## SCALING THE DATA
preProcValues <- preProcess(test_f[, num_cols], method=c('center','scale'))
test_f[, num_cols] <- predict(preProcValues, test_f[,num_cols])

test_f$BikeBuyer <- adventure$BikeBuyer[1:500]

test_final <- predict(dummies, newdata=test_f)
test_final_20 = test_final %*% as.matrix(scaled_pca_20)
test_final_20 = data.frame(test_final_20)
## since we don't actually have the 'BikeBuyer' label, I skipped one step from teh example.



test_final_20$probs = predict(logistic_mod_20, newdata=test_final_20, type='response')
test_final_20= score_model(test_final_20, 0.4)
write.csv(test_final_20,"test_final_20.csv")
